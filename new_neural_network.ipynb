{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv')",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "89b1de9c9eff3a99891b06a3f14a8dfa90e49b5ddc6450870190f7f479004b7e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tensorflow.python.keras import backend as K\n",
    "from audio import read_mfcc\n",
    "from batcher import sample_from_mfcc\n",
    "from constants import SAMPLE_RATE, NUM_FRAMES\n",
    "from conv_models import DeepSpeakerModel\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = 'revised_audio'\n",
    "SOURCE_DIR = 'accents_features'\n",
    "MODEL_PATH = 'model.pt'\n",
    "\n",
    "TRAIN_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \"\"\"Load numpy files from directory structure where each numpy file represents\n",
    "    the extracted features from the pre-trained model\"\"\"\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        outputs = []\n",
    "        labels = []\n",
    "\n",
    "        speakers = [f for f in os.listdir(directory) if f != '.DS_Store']\n",
    "        for i, speaker in enumerate(speakers):\n",
    "            for clip in os.listdir(f'{directory}/{speaker}'):\n",
    "                if 'npy' not in clip:\n",
    "                    continue\n",
    "\n",
    "                output = np.load(f'{directory}/{speaker}/{clip}')\n",
    "\n",
    "                outputs.append(output)\n",
    "                labels.append(i)\n",
    "\n",
    "        self.outputs = np.array(outputs)\n",
    "        self.labels = np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.outputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.outputs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dir = f'{AUDIO_PATH}/{SOURCE_DIR}'\n",
    "full_dataset = ClassifierDataset(dataset_dir)\n",
    "\n",
    "classes = [f for f in os.listdir(dataset_dir) if f != '.DS_Store']\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_size = int(TRAIN_SPLIT * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Define a simple linear neural network\n",
    "\n",
    "    Args:\n",
    "        num_classes: the number of classes we are classifying\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(classifier_training_loader, classifier_validation_loader, num_classes, num_epochs=150, lr=0.003, use_checkpoint=False):\n",
    "    classifier = Classifier(num_classes=num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=lr)\n",
    "    initial_epoch_count = 0\n",
    "\n",
    "    if use_checkpoint:\n",
    "        print('INFO: Loading state from latest saved model')\n",
    "        checkpoint = torch.load(MODEL_PATH)\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        initial_epoch_count = checkpoint['epoch']\n",
    "        print(f'INFO: Beginning from epoch {initial_epoch_count}')\n",
    "\n",
    "\n",
    "    \n",
    "    #weights = [subject_weight] * num_classes\n",
    "    #weights[-1] = junk_weight\n",
    "    #weights = torch.from_numpy(np.array(weights)).type(torch.FloatTensor)\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss(weight=weights, reduction='mean')\n",
    "    # disable weights when we aren't using junk\n",
    "\n",
    "\n",
    "    for epoch_num, epoch in enumerate(range(num_epochs)):\n",
    "        classifier.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_index, (inputs, labels) in enumerate(classifier_training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_index % 120 == 119:\n",
    "                print(f'[{initial_epoch_count + epoch_num + 1}, {batch_index + 1}]: loss: {running_loss / 120}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        classifier.eval()\n",
    "        validation_loss = 0.0\n",
    "        for batch_index, (inputs, labels) in enumerate(classifier_validation_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = classifier(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            validation_loss += loss.item()\n",
    "            if batch_index % 120 == 119:\n",
    "                print(f'[{initial_epoch_count + epoch_num + 1}, {batch_index + 1}]: loss: {validation_loss / 120}')\n",
    "                validation_loss = 0.0\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': initial_epoch_count + epoch_num,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, MODEL_PATH)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO: Loading state from latest saved model\n",
      "INFO: Beginning from epoch 9\n",
      "[10, 120]: loss: 6.901405115922292\n",
      "[10, 240]: loss: 6.927286048730214\n",
      "[10, 360]: loss: 6.95398440361023\n",
      "[10, 480]: loss: 6.951860415935516\n",
      "[10, 120]: loss: 6.959292741616567\n",
      "[10, 240]: loss: 6.971321340401968\n",
      "[10, 360]: loss: 6.972826302051544\n",
      "[10, 480]: loss: 6.995884430408478\n",
      "[10, 600]: loss: 7.020281755924225\n",
      "[10, 720]: loss: 7.017266647020976\n",
      "[10, 840]: loss: 7.025082890192667\n",
      "[10, 960]: loss: 7.0525570193926495\n",
      "[10, 1080]: loss: 7.031791671117147\n",
      "[10, 1200]: loss: 7.02896560827891\n",
      "[10, 1320]: loss: 7.039241194725037\n",
      "[10, 1440]: loss: 7.0316669821739195\n",
      "[10, 1560]: loss: 7.054549912611644\n",
      "[10, 1680]: loss: 7.063212474187215\n",
      "[10, 1800]: loss: 7.059494523207347\n",
      "[10, 1920]: loss: 7.044397628307342\n",
      "[11, 120]: loss: 6.893065094947815\n",
      "[11, 240]: loss: 6.917032265663147\n",
      "[11, 360]: loss: 6.942918682098389\n",
      "[11, 480]: loss: 6.942930515607198\n",
      "[11, 120]: loss: 6.952178676923116\n",
      "[11, 240]: loss: 6.96568900346756\n",
      "[11, 360]: loss: 6.967139077186585\n",
      "[11, 480]: loss: 6.988136645158132\n",
      "[11, 600]: loss: 7.013671294848124\n",
      "[11, 720]: loss: 7.010999937852223\n",
      "[11, 840]: loss: 7.018332235018412\n",
      "[11, 960]: loss: 7.042935343583425\n",
      "[11, 1080]: loss: 7.025396398703257\n",
      "[11, 1200]: loss: 7.0236932595570885\n",
      "[11, 1320]: loss: 7.031850560506185\n",
      "[11, 1440]: loss: 7.024276006221771\n",
      "[11, 1560]: loss: 7.048202502727508\n",
      "[11, 1680]: loss: 7.053556076685587\n",
      "[11, 1800]: loss: 7.053859261671702\n",
      "[11, 1920]: loss: 7.036996980508168\n",
      "[12, 120]: loss: 6.886493416627248\n",
      "[12, 240]: loss: 6.911951915423075\n",
      "[12, 360]: loss: 6.936173756917317\n",
      "[12, 480]: loss: 6.935486272970835\n",
      "[12, 120]: loss: 6.947337202231089\n",
      "[12, 240]: loss: 6.961250984668732\n",
      "[12, 360]: loss: 6.962860031922658\n",
      "[12, 480]: loss: 6.982997858524323\n",
      "[12, 600]: loss: 7.007831621170044\n",
      "[12, 720]: loss: 7.005451512336731\n",
      "[12, 840]: loss: 7.012641783555349\n",
      "[12, 960]: loss: 7.038659890492757\n",
      "[12, 1080]: loss: 7.020326650142669\n",
      "[12, 1200]: loss: 7.019773530960083\n",
      "[12, 1320]: loss: 7.027191213766733\n",
      "[12, 1440]: loss: 7.019998077551524\n",
      "[12, 1560]: loss: 7.042399454116821\n",
      "[12, 1680]: loss: 7.049500481287638\n",
      "[12, 1800]: loss: 7.049621085325877\n",
      "[12, 1920]: loss: 7.032471152146657\n",
      "[13, 120]: loss: 6.880297629038493\n",
      "[13, 240]: loss: 6.9061076760292055\n",
      "[13, 360]: loss: 6.932158172130585\n",
      "[13, 480]: loss: 6.932363931337992\n",
      "[13, 120]: loss: 6.944832714398702\n",
      "[13, 240]: loss: 6.955821386973063\n",
      "[13, 360]: loss: 6.957794936498006\n",
      "[13, 480]: loss: 6.9795453985532125\n",
      "[13, 600]: loss: 7.005262033144633\n",
      "[13, 720]: loss: 7.00021812915802\n",
      "[13, 840]: loss: 7.010170634587606\n",
      "[13, 960]: loss: 7.034760646025339\n",
      "[13, 1080]: loss: 7.017210284868876\n",
      "[13, 1200]: loss: 7.01621599594752\n",
      "[13, 1320]: loss: 7.022662182648976\n",
      "[13, 1440]: loss: 7.016873653729757\n",
      "[13, 1560]: loss: 7.038823095957438\n",
      "[13, 1680]: loss: 7.045700208346049\n",
      "[13, 1800]: loss: 7.046069665749868\n",
      "[13, 1920]: loss: 7.029152361551921\n",
      "[14, 120]: loss: 6.873022329807282\n",
      "[14, 240]: loss: 6.903169111410777\n",
      "[14, 360]: loss: 6.928893947601319\n",
      "[14, 480]: loss: 6.928213556607564\n",
      "[14, 120]: loss: 6.940548733870188\n",
      "[14, 240]: loss: 6.951739124457041\n",
      "[14, 360]: loss: 6.952916204929352\n",
      "[14, 480]: loss: 6.975728837649028\n",
      "[14, 600]: loss: 7.000939671198527\n",
      "[14, 720]: loss: 6.994482127825419\n",
      "[14, 840]: loss: 7.006982322533926\n",
      "[14, 960]: loss: 7.030179111162822\n",
      "[14, 1080]: loss: 7.01510904232661\n",
      "[14, 1200]: loss: 7.011700801054636\n",
      "[14, 1320]: loss: 7.018961159388224\n",
      "[14, 1440]: loss: 7.01131276289622\n",
      "[14, 1560]: loss: 7.036352487405141\n",
      "[14, 1680]: loss: 7.040736746788025\n",
      "[14, 1800]: loss: 7.038909606138865\n",
      "[14, 1920]: loss: 7.0245495994885765\n",
      "[15, 120]: loss: 6.867665938536326\n",
      "[15, 240]: loss: 6.9004922866821286\n",
      "[15, 360]: loss: 6.924415640036265\n",
      "[15, 480]: loss: 6.924091068903605\n",
      "[15, 120]: loss: 6.937558929125468\n",
      "[15, 240]: loss: 6.949093592166901\n",
      "[15, 360]: loss: 6.948730099201202\n",
      "[15, 480]: loss: 6.972768020629883\n",
      "[15, 600]: loss: 6.995376880963644\n",
      "[15, 720]: loss: 6.989253739515941\n",
      "[15, 840]: loss: 7.001916408538818\n",
      "[15, 960]: loss: 7.025104705492655\n",
      "[15, 1080]: loss: 7.009074751536051\n",
      "[15, 1200]: loss: 7.007800757884979\n",
      "[15, 1320]: loss: 7.015398073196411\n",
      "[15, 1440]: loss: 7.004416426022847\n",
      "[15, 1560]: loss: 7.031227072079976\n",
      "[15, 1680]: loss: 7.035236644744873\n",
      "[15, 1800]: loss: 7.034295924504598\n",
      "[15, 1920]: loss: 7.019643072287241\n",
      "[16, 120]: loss: 6.8622066100438435\n",
      "[16, 240]: loss: 6.896230892340342\n",
      "[16, 360]: loss: 6.919713191191355\n",
      "[16, 480]: loss: 6.921447547276815\n",
      "[16, 120]: loss: 6.934326521555582\n",
      "[16, 240]: loss: 6.947177926699321\n",
      "[16, 360]: loss: 6.946371575196584\n",
      "[16, 480]: loss: 6.96990065574646\n",
      "[16, 600]: loss: 6.99261071284612\n",
      "[16, 720]: loss: 6.985620562235514\n",
      "[16, 840]: loss: 6.997426617145538\n",
      "[16, 960]: loss: 7.023642237981161\n",
      "[16, 1080]: loss: 7.006401852766673\n",
      "[16, 1200]: loss: 7.005467116832733\n",
      "[16, 1320]: loss: 7.012509922186534\n",
      "[16, 1440]: loss: 7.001172327995301\n",
      "[16, 1560]: loss: 7.028200948238373\n",
      "[16, 1680]: loss: 7.0324979821840925\n",
      "[16, 1800]: loss: 7.031503788630167\n",
      "[16, 1920]: loss: 7.015863974889119\n",
      "[17, 120]: loss: 6.859403494993845\n",
      "[17, 240]: loss: 6.8951857407887776\n",
      "[17, 360]: loss: 6.9163303454717004\n",
      "[17, 480]: loss: 6.9198965430259705\n",
      "[17, 120]: loss: 6.931482156117757\n",
      "[17, 240]: loss: 6.945231421788534\n",
      "[17, 360]: loss: 6.943810284137726\n",
      "[17, 480]: loss: 6.96846741437912\n",
      "[17, 600]: loss: 6.988773930072784\n",
      "[17, 720]: loss: 6.983793604373932\n",
      "[17, 840]: loss: 6.995386564731598\n",
      "[17, 960]: loss: 7.021637137730917\n",
      "[17, 1080]: loss: 7.004447694619497\n",
      "[17, 1200]: loss: 7.003128099441528\n",
      "[17, 1320]: loss: 7.009896159172058\n",
      "[17, 1440]: loss: 6.99954047203064\n",
      "[17, 1560]: loss: 7.0264769474665325\n",
      "[17, 1680]: loss: 7.030302504698436\n",
      "[17, 1800]: loss: 7.030083624521891\n",
      "[17, 1920]: loss: 7.0146904190381365\n",
      "[18, 120]: loss: 6.856927760442098\n",
      "[18, 240]: loss: 6.894324088096619\n",
      "[18, 360]: loss: 6.915001622835795\n",
      "[18, 480]: loss: 6.918926974137624\n",
      "[18, 120]: loss: 6.930193030834198\n",
      "[18, 240]: loss: 6.943919372558594\n",
      "[18, 360]: loss: 6.942577123641968\n",
      "[18, 480]: loss: 6.966582477092743\n",
      "[18, 600]: loss: 6.9867977698644\n",
      "[18, 720]: loss: 6.981854748725891\n",
      "[18, 840]: loss: 6.993593080838521\n",
      "[18, 960]: loss: 7.0198265989621484\n",
      "[18, 1080]: loss: 7.003767514228821\n",
      "[18, 1200]: loss: 7.002223594983419\n",
      "[18, 1320]: loss: 7.009092013041179\n",
      "[18, 1440]: loss: 6.998120709260305\n",
      "[18, 1560]: loss: 7.024267800649008\n",
      "[18, 1680]: loss: 7.029275969664256\n",
      "[18, 1800]: loss: 7.029385590553284\n",
      "[18, 1920]: loss: 7.014104203383128\n",
      "[19, 120]: loss: 6.854266063372294\n",
      "[19, 240]: loss: 6.893552207946778\n",
      "[19, 360]: loss: 6.91375036239624\n",
      "[19, 480]: loss: 6.917828730742136\n",
      "[19, 120]: loss: 6.928270999590556\n",
      "[19, 240]: loss: 6.942299691836039\n",
      "[19, 360]: loss: 6.9415429592132565\n",
      "[19, 480]: loss: 6.965274675687154\n",
      "[19, 600]: loss: 6.985017836093903\n",
      "[19, 720]: loss: 6.9810583670934045\n",
      "[19, 840]: loss: 6.99051505724589\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-c0e3ebd1f91e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-29d59df1e8b8>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(classifier_training_loader, classifier_validation_loader, num_classes, num_epochs, lr, use_checkpoint)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/automatic-speaker-recognition/venv/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/automatic-speaker-recognition/venv/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_classifier = train_classifier(train_loader, test_loader, num_classes=len(classes), num_epochs=10000, use_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def test_classifier(classifier, classifier_testing_loader, count, output_stats=False):\n",
    "    class_correct = [0] * count\n",
    "    class_total = [0] * count\n",
    "    \n",
    "    # used to calculate global f1\n",
    "    all_labels = []\n",
    "    all_predicted = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in classifier_testing_loader:\n",
    "            images, labels = data\n",
    "            outputs = classifier(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            \n",
    "            all_labels += labels\n",
    "            all_predicted += predicted\n",
    "\n",
    "    f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
    "\n",
    "    if output_stats:\n",
    "        print(f'f1: {f1}')\n",
    "        \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f1: 0.9926611587401639\n"
     ]
    }
   ],
   "source": [
    "# prev: f1: 0.8805042046315826\n",
    "classifier = Classifier(num_classes=len(classes))\n",
    "checkpoint = torch.load(MODEL_PATH)\n",
    "classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "classifier.eval()\n",
    "f1 = test_classifier(classifier, test_loader, len(classes), output_stats=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}